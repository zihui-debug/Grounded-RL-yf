# llm_judge.py

from __future__ import annotations
import base64
from io import BytesIO
from typing import Optional, List
import math
from PIL import Image
import torch
import ast
import math

from transformers import (
    StoppingCriteria,
)
import openai
from openai import AzureOpenAI, AsyncAzureOpenAI
import os
import logging
import wandb  # Added import for Weights & Biases

logger = logging.getLogger(__name__)

try:
    client = AzureOpenAI(
        api_key = os.getenv("AZURE_OPENAI_KEY"),  
        api_version = "2023-05-15",
        azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
    )
except openai.OpenAIError:
    logger.warning("Failed to initialized Azure OpenAI client. This may lead to errors if using Azure OpenAI for model inference.")

try:
    from qwen_vl_utils import process_vision_info
except ImportError:
    logger.warning("Failed to import qwen_vl_utils; Please install it via `pip install qwen-vl-utils`")

class StopOnTokens(StoppingCriteria):
    """
    A custom stopping criterion to stop generation when any one of the provided tokens appears.
    """
    def __init__(self, stop_ids: list):
        self.stop_ids = stop_ids

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        # For multi-beam, input_ids is (batch_size * num_beams, current_length).
        # Check the last token in each beam; if any is in stop_ids, stop.
        for sequence in input_ids:
            if sequence[-1] in self.stop_ids:
                return True
        return False

class Judge:
    
    """
    Represents a judge model.
    
    INPUTS:
    - judge_llm_wrapper: The LLM model to be used as a judge.
    - judge_system_prompt: The prompt to be used for the judge model.
    - wandb_project: The Weights & Biases project name.
    - use_wandb: Whether to use Weights & Biases.
    
    METHODS:
    - judge(gt_ans: str, pred_ans: str, choices: str = None, image: Image = None) -> float: Judges the predicted answer and returns a float score. 1.0 for the correct answer, 0.0 for the wrong answer.
    
    """

    def __init__(
            self,
            judge_llm_wrapper,
            judge: str,
            wandb_project: str = "vlm-search",
            use_wandb: bool = False,
            temperature: float = 0.,
            point_matching_threshold: float = 50,
            thought_token_begin: str = "<think>",
            thought_token_end: str = "</think>",
            final_token_begin: str = "<answer>",
            final_token_end: str = "</answer>",

    ):
        self.judge_llm = judge_llm_wrapper
        self.judge_temperature = temperature
        self.judge_type = judge
        self.point_matching_threshold = point_matching_threshold
        self.thought_token_begin = thought_token_begin
        self.thought_token_end = thought_token_end
        self.final_token_begin = final_token_begin
        self.final_token_end = final_token_end

        self.judge_system_prompt = (
            "You are a reliable judge. "
            "Reply with only either of the following 2 numbers: 1 for correct or 0 for incorrect. "
            "You will be given the 'Input Question', 'Original Image', ’Ground Truth Answer’, ’Predicted Answer’ "
            "You will judge the 'Predicted Answer' generated by the assistant by comparing it with 'Ground Truth Answer'. "
            "If the answer is correct, return 1. If the answer is incorrect, return 0."
        )

        if use_wandb:
            wandb.init(project=wandb_project, mode="disabled")
    
    def _encode_image(self, image: Image.Image) -> str:
        """
        Converts a PIL Image to base64-encoded JPEG for Qwen2-VL's 'image' content.
        """
        buffer = BytesIO()
        image.convert("RGB").save(buffer, format="JPEG")
        base64_bytes = base64.b64encode(buffer.getvalue())
        return base64_bytes.decode("utf-8")


    #add a judge function just checks the formatting of the response.

    def format_check(self, pred_thought_node: str) -> bool:

        """Checks the formatting of the predicted answer matches the expected format.
        
        INPUTS:
        - pred_ans_full: The full predicted answer.
        
        OUTPUTS:
        - is_correct: Whether the formatting is correct."""
        
        if self.thought_token_begin not in pred_thought_node:
            return False
        if self.thought_token_begin in pred_thought_node and self.thought_token_end not in pred_thought_node:
            return False
        
        return True
    

    def judge_gpt4o(self, messages : list) -> float:

        logger.debug(f"LLM Judge: Using GPT-4o model.")

        completion = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            max_tokens=1024,
            temperature=self.judge_temperature,
            top_p=1.0,
        )
        return completion.choices[0].message.content
    
    def judge_qwen2(self, messages : list) -> float:

        logger.debug(f"LLM Judge: Using GPT-4o model.")

        conversations = [messages]

        #Convert conversation to text that Qwen2-VL uses, while also extracting image/video
        texts = [self.judge_llm.processor.apply_chat_template(conv, tokenize=False, add_generation_prompt=True) 
                 for conv in conversations]
        
        # Let Qwen handle the images via process_vision_info
        image_inputs, video_inputs = process_vision_info(conversations)

        # Now build the model inputs
        inputs = self.judge_llm.processor(
            text=texts,
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )

        # Move inputs onto the correct device
        inputs = inputs.to(self.judge_llm._device)

        # Prepare generation settings
        pad_token_id = self.judge_llm.tokenizer.pad_token_id

        

        logger.debug(f"Generating with temperature {self.judge_temperature}")
        logger.debug(f"Generating with top_p {self.judge_llm.top_p}")
        logger.debug(f"Generating with use_cache {self.judge_llm.use_cache}")
        logger.debug(f"Generating with num_beams {self.judge_llm.num_beams}")
        logger.debug(f"Generating with max_new_tokens {self.judge_llm.max_new_tokens}")
        logger.debug(f"Generating with top_k {self.judge_llm.top_k}")

        # Actually run generate
        #set the temperature to 0.0, and remove the top_k
        outputs = self.judge_llm.model.generate(
            **inputs,
            eos_token_id=self.judge_llm.tokenizer.eos_token_id,
            pad_token_id=pad_token_id,
            max_new_tokens=self.judge_llm.max_new_tokens,
            temperature=self.judge_temperature,
            top_p=self.judge_llm.top_p,
            use_cache=self.judge_llm.use_cache,
            do_sample=True if self.judge_llm.temperature > 0 else False,
            # stopping_criteria=stopping_criteria,
            # num_beam_groups=2,
            # num_beams=2,
            # diversity_penalty=10.0,
            # top_k=self.judge_llm.top_k,
        )

        # Separate out the newly generated tokens from the prompt
        generated_ids_trimmed = []
        for in_ids, out_ids in zip(inputs.input_ids, outputs):
            generated_ids_trimmed.append(out_ids[len(in_ids) :])
        
        # Decode them
        answers = self.judge_llm.processor.batch_decode(
            generated_ids_trimmed, 
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )
        
        # Extract the judge's response
        judge_response = answers[0]

        return judge_response

    
    def judge(
        self, 
        gt_ans: str,
        pred_ans: str, 
        input_query: str, 
        choices: str = None, 
        image: Image = None,
        scale_factor: float = 1.0
        ) -> float:

        """
        Judges the predicted answer and returns a float score. 1.0 for the correct answer, 0.0 for the wrong answer.
        
        INPUTS:
        - gt_ans: The ground truth answer.
        - pred_ans: The predicted answer.
        - image: The image associated with the question.
        
        OUTPUTS:
        - score: The score of the predicted answer.
        
        """

        #check for the special case where the GT answer is just "yes" or "no"
        #remove any leading or trailing whitespaces and punctuations 
        #do string matching and return the score

        gt_ans_copy = gt_ans

        if self.judge_type == "point_match":
            return self._point_matching(gt_ans_copy, pred_ans, self.point_matching_threshold, scale_factor)

        if self.judge_type == "string_match":
            # print(f"gt_ans_copy: {gt_ans_copy}, pred_ans: {pred_ans}, score: {self._string_matching(gt_ans_copy, pred_ans)}")
            return self._string_matching(gt_ans_copy, pred_ans)

        if self.judge_type == "point_in_bbox":
            return self._point_in_bbox(gt_ans_copy, pred_ans, scale_factor)

        if self.judge_type == "web_action":
            return self._web_action(gt_ans_copy, pred_ans)
        
        

        judge_response = None
        if self.judge_type == "gpt4o":
            # Build messages for the judge model
            messages = [
                {
                    "role": "system",
                    "content": self.judge_system_prompt
                }
            ]
            if image is not None:
                base64_string = self._encode_image(image)
                messages.append(
                    {
                        "role": "user",
                        "content": [
                            {"type": "image_url", "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_string}"}
                            }
                        ]
                    }
                    )
            
            # Add the ground truth answer and predicted answer to the input query
            text_str = f"Input Question: {input_query}\n"
            text_str += f"Ground Truth Answer: {gt_ans}\n"
            text_str += f"Predicted Answer: {pred_ans}\n"
            if choices is not None:
                text_str += f'Choices: {choices}\n'

            # Add the user content to the messages
            messages.append({
                "role": "user",
                "content": [{"type": "text", "text": text_str}]
            })
            judge_response = self.judge_gpt4o(messages)
        elif self.judge_type == "qwen2":
            raise NotImplementedError("Qwen2 currently not supported.")
            user_content =[]

            #encode the image if it exists
            if image is not None:
                base64_string = self._encode_image(image)
                user_content.append({"type": "image", "image": f"data:image/jpeg;base64,{base64_string}"})
            
            # Add the ground truth answer and predicted answer to the input query
            text_str = f"Input Question: {input_query}\n"
            text_str += f"Ground Truth Answer: {gt_ans}\n"
            text_str += f"Predicted Answer: {pred_ans}\n"
            if choices is not None:
                text_str += f'Choices: {choices}\n'
            user_content.append({"type": "text", "text": text_str})

            # Add the user content to the messages
            messages.append({
                "role": "user",
                "content": user_content
            })
            judge_response = self.judge_qwen2(messages)

        elif self.judge_type == "qwen_vllm":
            logger.error("judge model set to Qwen VLLM, but this model is not implemented yet for judge.")
            raise NotImplementedError("Qwen VLLM is not implemented yet.")

        else:
            raise ValueError(f"Invalid judge: {self.judge_type}")
        
        if judge_response is None:
            raise ValueError("Invalid response from the judge model. The response should be either 0 or 1.")

        #check the format of the response
        judge_response = self._judge_format_check(judge_response, gt_ans, pred_ans, input_query, text_str, choices)

        return judge_response
    
    
    def _judge_format_check(self, judge_response, gt_ans: str, pred_ans: str, input_query: str, text_str: str, choices: str = None) -> float:

        #Just have a check that the judge response is just 0 1. 
        if judge_response == "0" or judge_response == "1":
            logger.debug(f"input_query: {input_query}")
            logger.debug(f"gt_ans: {gt_ans}")
            logger.debug(f"pred_ans: {pred_ans}")
            logger.debug(f"Judge response: {judge_response}")
            if choices is not None:
                logger.debug(f'choices: {choices}')
            return float(judge_response)
        
        else :
            if "0" in judge_response and "1" in judge_response:
                raise ValueError(f"Invalid response from the judge model. The response should be either 0 or 1.")
            
            elif "0" in judge_response:
                logger.debug(f"input_query: {input_query}")
                logger.debug(f"gt_ans: {gt_ans}")
                logger.debug(f"pred_ans: {pred_ans}")
                if choices is not None:
                    logger.debug(f'choices: {choices}')
                logger.debug(f"Judge Filtered response: {0}")
                logger.debug("Judge response: 0")
                return 0.0

            elif "1" in judge_response:
                logger.debug(f"input_query: {input_query}")
                logger.debug(f"gt_ans: {gt_ans}")
                logger.debug(f"pred_ans: {pred_ans}")
                if choices is not None:
                    logger.debug(f'choices: {choices}')
                logger.debug(f"Judge Filtered response: {1}")
                logger.debug("Judge response: 1")
                return 1.0
            else:
                logger.debug(f'judge response: {judge_response}')
                logger.debug(f'input: {text_str}')
                raise ValueError(f"Invalid response from the judge model. The response should be either 0 or 1.")

    def _web_action(self, gt_ans: str, pred_ans: str) -> float:
        """
        Judges the predicted answer and returns a float score. 1.0 for the correct answer, 0.0 for the wrong answer.
        """
        gt_ans = gt_ans.strip().lower()
        pred_ans = pred_ans.strip().lower()

        print(f"gt_ans: {gt_ans}, pred_ans: {pred_ans}")

        try:
            if "go_back" in pred_ans:
                return 1.0 if "go_back" in gt_ans else 0.0
            elif "stop" in pred_ans:
                return 1.0 if "stop" in gt_ans else 0.0
            elif "scroll" in pred_ans:
                return 1.0 if "scroll" in gt_ans else 0.0
            elif "click" in pred_ans:
                action, element_id = pred_ans.split(" ")
                action_match = "click" in gt_ans
                element_id_match = element_id.strip() in gt_ans
                return action_match and element_id_match
            elif "type" in pred_ans:
                action, element_id, content = pred_ans.split(" ")
                action_match = "type" in gt_ans
                element_id_match = element_id.strip() in gt_ans
                content_match = content.strip() in gt_ans
                return action_match and element_id_match
            else:
                return 0.0
        except Exception as e:
            return 0.0
    
    def _point_in_bbox(self, gt_ans: str, pred_ans: str, scale_factor: float) -> float:
        """
        Checks if a predicted point is within a ground truth bounding box.
        
        INPUTS:
        - gt_ans: The ground truth answer as a string representing a bounding box in format "(x1, y1, x2, y2)"
                where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner.
        - pred_ans: The predicted answer as a string representing a point in format "(x, y)".
        
        OUTPUTS:
        - score: 1.0 if the point is inside the bounding box, 0.0 otherwise.
        """

        # Attempt to parse the strings into tuple objects
        try:
            gt_bbox = tuple(ast.literal_eval(gt_ans))
            pred_point = tuple(ast.literal_eval(pred_ans))
        except (ValueError, SyntaxError, TypeError):
            logger.debug(f"Error parsing gt_ans: {gt_ans}, pred_ans: {pred_ans}")
            # If parsing fails, return 0.0
            return 0.0
        
        # Check if gt_bbox is a tuple with 4 integers (x1, y1, x2, y2)
        if not (isinstance(gt_bbox, tuple) and len(gt_bbox) == 4 and 
                all(isinstance(x, int) for x in gt_bbox)):
            logger.debug(f"Invalid bounding box format: {gt_bbox}")
            return 0.0
        
        # Check if pred_point is a tuple with 2 integers (x, y)
        if not (isinstance(pred_point, tuple) and len(pred_point) == 2 and 
                all(isinstance(x, int) for x in pred_point)):
            logger.debug(f"Invalid point format: {pred_point}")
            return 0.0
        
        # Extract coordinates
        x1, y1, x2, y2 = gt_bbox
        # x1, y1, x2, y2 = x1 * scale_factor, y1 * scale_factor, x2 * scale_factor, y2 * scale_factor

        x, y = pred_point
        
        # Check if the point is inside the bounding box
        if x1 <= x <= x2 and y1 <= y <= y2:
            logger.debug(f"Point {pred_point} is inside bbox {gt_bbox}")
            return 1.0
        else:
            logger.debug(f"Point {pred_point} is outside bbox {gt_bbox}")
            return 0.0

    def _point_matching(self, gt_ans: str, pred_ans: str, point_matching_threshold: float, scale_factor: float) -> float:
        """
        Attempts to parse gt_ans and pred_ans as tuples containing integers. 
        If parsing succeeds and both tuples are valid (all ints), computes their
        Euclidean distance and returns 1.0 if the distance is less than 
        point_matching_threshold, otherwise 0.0. If parsing or validation fails, returns 0.0.
        """

        # Attempt to parse the strings into tuple objects
        try:
            gt_tuple = ast.literal_eval(gt_ans)
            pred_tuple = ast.literal_eval(pred_ans)
        except (ValueError, SyntaxError, TypeError):
            logger.debug(f"Error parsing gt_ans: {gt_ans}, pred_ans: {pred_ans}")
            # If parsing fails, return 0.0
            return 0.0

        # Check if they are tuples
        if not (isinstance(gt_tuple, tuple) and isinstance(pred_tuple, tuple)):
            logger.debug(f"gt_tuple: {gt_tuple}, pred_tuple: {pred_tuple}")
            return 0.0

        # Check if all elements in both tuples are integers
        if not all(isinstance(x, int) for x in gt_tuple) or not all(isinstance(x, int) for x in pred_tuple):
            logger.debug(f"gt_tuple: {gt_tuple}, pred_tuple: {pred_tuple}")
            return 0.0

        # Check that they have the same size for distance calculation
        if len(gt_tuple) != len(pred_tuple):
            logger.debug(f"gt_tuple: {gt_tuple}, pred_tuple: {pred_tuple}")
            return 0.0

        # Compute Euclidean distance
        distance = math.dist(gt_tuple, pred_tuple)
        # Return 1.0 if distance is less than threshold, otherwise 0.0
        return 1.0 if distance < point_matching_threshold else 0.0

    

    def _string_matching(self, gt_ans: str, pred_ans: str) -> float:

        """
        Judges the predicted answer and returns a float score. 1.0 for the correct answer, 0.0 for the wrong answer.
        
        INPUTS:
        - gt_ans: The ground truth answer.
        - pred_ans: The predicted answer.
        
        OUTPUTS:
        - score: The score of the predicted answer.
        
        """

        #Accounitng for the fact that the answers may be in different cases
        gt_ans = gt_ans.lower()
        pred_ans = pred_ans.lower()

        pred_ans = self._remove_punctuation_spaces(pred_ans)
        gt_ans = self._remove_punctuation_spaces(gt_ans)

        if gt_ans == pred_ans or gt_ans in pred_ans:
            logger.debug(f"Judge response : 1.0")
            return 1.0
        else:
            logger.debug(f"Judge response : 0.0")
            return 0.0
    
    def _check_gt_ans_yes_no(self, gt_ans: str) -> bool:

        """
        Checks if the ground truth answer is a yes or no answer.
        
        INPUTS:
        - gt_ans: The ground truth answer.
        
        OUTPUTS:
        - is_yes_no: Whether the ground truth answer is a yes or no answer.
        
        """

        #Accounitng for the fact that the answers may be in different cases
        gt_ans = gt_ans.lower()

        gt_ans = self._remove_punctuation_spaces(gt_ans)
        
        if gt_ans == "yes" or gt_ans == "no":
            return True
        else:
            return False
    
    def _remove_punctuation_spaces(self, ans: str) -> str:

        """
        Removes punctuation from the answer and leading and trailing spaces.
        
        INPUTS:
        - ans: The answer.
        
        OUTPUTS:
        - ans_filtered: The answer without punctuation.
        
        """

        #remove any spaces 
        ans = ans.strip()

        #remove any punctuation marks
        ans_filtered = ans.replace(".", "")
        ans_filtered = ans_filtered.replace("?", "")
        ans_filtered = ans_filtered.replace("!", "")
        ans_filtered = ans_filtered.replace(",", "")
        ans_filtered = ans_filtered.replace(";", "")
        ans_filtered = ans_filtered.replace(":", "")
        ans_filtered = ans_filtered.replace("'", "")
        ans_filtered = ans_filtered.replace('"', "")
        ans_filtered = ans_filtered.replace("(", "")
        ans_filtered = ans_filtered.replace(")", "")
        ans_filtered = ans_filtered.replace("[", "")
        ans_filtered = ans_filtered.replace("]", "")
        ans_filtered = ans_filtered.replace("{", "")
        ans_filtered = ans_filtered.replace("}", "")
        ans_filtered = ans_filtered.replace("<", "")
        ans_filtered = ans_filtered.replace(">", "")
        ans_filtered = ans_filtered.replace("/", "")
        ans_filtered = ans_filtered.replace("\\", "")
        ans_filtered = ans_filtered.replace("|", "")
        ans_filtered = ans_filtered.replace("=", "")
        ans_filtered = ans_filtered.replace("+", "")
        ans_filtered = ans_filtered.replace("-", "")
        ans_filtered = ans_filtered.replace("_", "")
        ans_filtered = ans_filtered.replace("*", "")
        ans_filtered = ans_filtered.replace("&", "")
        ans_filtered = ans_filtered.replace("^", "")
        ans_filtered = ans_filtered.replace("%", "")
        ans_filtered = ans_filtered.replace("$", "")
        ans_filtered = ans_filtered.replace("#", "")
        ans_filtered = ans_filtered.replace("@", "")
        ans_filtered = ans_filtered.replace("`", "")
        ans_filtered = ans_filtered.replace("~", "")
        ans_filtered = ans_filtered.replace(" ", "")
        ans_filtered = ans_filtered.strip()
        ans_filtered = ans_filtered.lower()

        return ans_filtered